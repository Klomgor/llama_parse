{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Parse ‚Üí Classify ‚Üí Extract Workflow with LlamaCloud Services\n",
    "\n",
    "This notebook demonstrates the complete workflow for processing documents using LlamaCloud services:\n",
    "1. **Parse** - Extract and convert documents to markdown\n",
    "2. **Classify** - Categorize documents based on their content\n",
    "3. **Extract** - Extract structured data using the markdown as input via SourceText\n",
    "\n",
    "## Overview of the Workflow\n",
    "\n",
    "### 1. Parse Phase\n",
    "- Use `LlamaParse` to convert documents (PDFs, Word docs, etc.) into structured formats\n",
    "- Extract markdown content that preserves document structure\n",
    "- Get both raw text and markdown representations\n",
    "\n",
    "### 2. Classify Phase\n",
    "- Use `ClassifyClient` to categorize documents based on content\n",
    "- Apply classification rules to route documents appropriately\n",
    "- Handle different document types with specific processing logic\n",
    "\n",
    "### 3. Extract Phase\n",
    "- Use `LlamaExtract` with `SourceText` to extract structured data\n",
    "- Pass the markdown content as input for more accurate extraction\n",
    "- Define custom schemas for structured data extraction\n",
    "\n",
    "Let's walk through each step with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-cloud-services\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set up API key\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"\"  # edit it\n",
    "\n",
    "# Setup Base URL\n",
    "# os.envrion[\"LLAMA_CLOUD_BASE_URL\"] = \"https://api.cloud.eu.llamaindex.ai/\" # update if necessay\n",
    "\n",
    "print(\"‚úÖ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Sample Documents\n",
    "\n",
    "Let's download some sample documents to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ financial_report.pdf already exists\n",
      "üìÅ technical_spec.pdf already exists\n",
      "\n",
      "üìÇ Sample documents ready!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Create directory for sample documents\n",
    "os.makedirs(\"sample_docs\", exist_ok=True)\n",
    "\n",
    "# Download sample documents\n",
    "docs_to_download = {\n",
    "    \"financial_report.pdf\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\",\n",
    "    \"technical_spec.pdf\": \"https://www.ti.com/lit/ds/symlink/lm317.pdf\",\n",
    "}\n",
    "\n",
    "for filename, url in docs_to_download.items():\n",
    "    filepath = f\"sample_docs/{filename}\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"‚úÖ Downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to download {filename}\")\n",
    "    else:\n",
    "        print(f\"üìÅ {filename} already exists\")\n",
    "\n",
    "print(\"\\nüìÇ Sample documents ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Document Parsing\n",
    "\n",
    "First, let's parse our documents using LlamaParse to extract clean markdown content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Parsing documents...\n",
      "Started parsing the file under job_id 8a8c76f9-354d-4275-91d8-312ff1adc762\n",
      "...‚úÖ Parsed financial report (Job ID: 8a8c76f9-354d-4275-91d8-312ff1adc762)\n",
      "Started parsing the file under job_id 7e603448-ed80-4d18-948b-6801ed51c41b\n",
      "‚úÖ Parsed technical spec (Job ID: 7e603448-ed80-4d18-948b-6801ed51c41b)\n",
      "\n",
      "üìÑ Parsing complete!\n"
     ]
    }
   ],
   "source": [
    "from llama_cloud_services.parse.base import LlamaParse\n",
    "from llama_cloud_services.parse.utils import ResultType\n",
    "import asyncio\n",
    "\n",
    "# Initialize the parser\n",
    "parser = LlamaParse(\n",
    "    result_type=ResultType.MD,  # Get markdown output\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    "    # Premium mode for better accuracy\n",
    "    premium_mode=True,\n",
    "    # Extract tables as HTML for better structure\n",
    "    output_tables_as_HTML=True,\n",
    "    # Parse only first few pages for demo\n",
    ")\n",
    "\n",
    "print(\"üîÑ Parsing documents...\")\n",
    "\n",
    "# Parse the financial report\n",
    "financial_result = await parser.aparse(\"sample_docs/financial_report.pdf\")\n",
    "print(f\"‚úÖ Parsed financial report (Job ID: {financial_result.job_id})\")\n",
    "\n",
    "# Parse the technical specification\n",
    "technical_result = await parser.aparse(\"sample_docs/technical_spec.pdf\")\n",
    "print(f\"‚úÖ Parsed technical spec (Job ID: {technical_result.job_id})\")\n",
    "\n",
    "print(\"\\nüìÑ Parsing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Markdown Content\n",
    "\n",
    "Now let's get the markdown content from our parsed documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Financial Report Markdown (first 500 chars):\n",
      "\n",
      "\n",
      "# UNITED STATES\n",
      "# SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "\n",
      "## FORM 10-K\n",
      "\n",
      "(Mark One)\n",
      "\n",
      "‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended December 31, 2021\n",
      "OR\n",
      "‚òê TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from_____ to _____\n",
      "Commission File Number: 001-38902\n",
      "\n",
      "# UBER TECHNOLOGIES, INC.\n",
      "(Exact name of registrant as specified in its charter)\n",
      "\n",
      "Delaware\n",
      "...\n",
      "\n",
      "üìã Technical Spec Markdown (first 500 chars):\n",
      "\n",
      "\n",
      "LM317\n",
      "SLVS044Z ‚Äì SEPTEMBER 1997 ‚Äì REVISED APRIL 2025\n",
      "\n",
      "# LM317 3-Pin Adjustable Regulator\n",
      "\n",
      "## 1 Features\n",
      "\n",
      "‚Ä¢ Output voltage range:\n",
      "  ‚Äì Adjustable: 1.25V to 37V\n",
      "‚Ä¢ Output current: 1.5A\n",
      "‚Ä¢ Line regulation: 0.01%/V (typ)\n",
      "‚Ä¢ Load regulation: 0.1% (typ)\n",
      "‚Ä¢ Internal short-circuit current limiting\n",
      "‚Ä¢ Thermal overload protection\n",
      "‚Ä¢ Output safe-area compensation (new chip)\n",
      "‚Ä¢ PSRR: 80dB at 120Hz for CADJ = 10ŒºF (new chip)\n",
      "‚Ä¢ Packages:\n",
      "  ‚Äì 4-pin, SOT-223 (DCY)\n",
      "  ‚Äì 3-pin, TO-263 (KTT)\n",
      "  ‚Äì 3-pin, TO-220 (KCS, KCT),\n",
      "...\n",
      "\n",
      "üìè Financial report markdown length: 1348671 characters\n",
      "üìè Technical spec markdown length: 90971 characters\n"
     ]
    }
   ],
   "source": [
    "# Get markdown content from parsed documents\n",
    "financial_markdown = await financial_result.aget_markdown()\n",
    "technical_markdown = await technical_result.aget_markdown()\n",
    "\n",
    "print(\"üìã Financial Report Markdown (first 500 chars):\")\n",
    "print(financial_markdown[:500])\n",
    "print(\"...\\n\")\n",
    "\n",
    "print(\"üìã Technical Spec Markdown (first 500 chars):\")\n",
    "print(technical_markdown[:500])\n",
    "print(\"...\\n\")\n",
    "\n",
    "print(f\"üìè Financial report markdown length: {len(financial_markdown)} characters\")\n",
    "print(f\"üìè Technical spec markdown length: {len(technical_markdown)} characters\")\n",
    "\n",
    "document_texts = [financial_markdown, technical_markdown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Document Classification\n",
    "\n",
    "Next, let's classify our documents based on their content using the ClassifyClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è  Setting up document classification...\n",
      "üìù Created 3 classification rules\n"
     ]
    }
   ],
   "source": [
    "from llama_cloud_services.beta.classifier.client import ClassifyClient\n",
    "from llama_cloud.types import ClassifierRule\n",
    "from llama_cloud_services.files.client import FileClient\n",
    "from llama_cloud.client import AsyncLlamaCloud\n",
    "\n",
    "# Initialize the classify client\n",
    "api_key = os.environ[\"LLAMA_CLOUD_API_KEY\"]\n",
    "classify_client = ClassifyClient.from_api_key(api_key)\n",
    "\n",
    "print(\"üè∑Ô∏è  Setting up document classification...\")\n",
    "\n",
    "# Define classification rules\n",
    "classification_rules = [\n",
    "    ClassifierRule(\n",
    "        type=\"financial_document\",\n",
    "        description=\"Documents containing financial data, revenue, expenses, SEC filings, or financial statements\",\n",
    "    ),\n",
    "    ClassifierRule(\n",
    "        type=\"technical_specification\",\n",
    "        description=\"Technical datasheets, component specifications, engineering documents, or technical manuals\",\n",
    "    ),\n",
    "    ClassifierRule(\n",
    "        type=\"general_document\",\n",
    "        description=\"General business documents, contracts, or other unspecified document types\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"üìù Created {len(classification_rules)} classification rules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Structured Data Extraction using SourceText\n",
    "\n",
    "Now comes the key part - using the markdown content as input for structured data extraction via SourceText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  LlamaExtract initialized\n"
     ]
    }
   ],
   "source": [
    "from llama_cloud_services.extract.extract import LlamaExtract, SourceText\n",
    "from llama_cloud.types import ExtractConfig, ExtractMode\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Initialize LlamaExtract\n",
    "llama_extract = LlamaExtract(api_key=api_key, verbose=True)\n",
    "\n",
    "print(\"‚öôÔ∏è  LlamaExtract initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Extraction Schemas\n",
    "\n",
    "Let's define different schemas for different document types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Extraction schemas defined\n"
     ]
    }
   ],
   "source": [
    "# Schema for financial documents\n",
    "class FinancialMetrics(BaseModel):\n",
    "    company_name: str = Field(description=\"Name of the company\")\n",
    "    document_type: str = Field(\n",
    "        description=\"Type of financial document (10-K, 10-Q, annual report, etc.)\"\n",
    "    )\n",
    "    fiscal_year: int = Field(description=\"Fiscal year of the report\")\n",
    "    revenue_2021: str = Field(description=\"Total revenue in 2021\")\n",
    "    net_income_2021: str = Field(description=\"Net income in 2021\")\n",
    "    key_business_segments: List[str] = Field(\n",
    "        default=[], description=\"Main business segments or divisions\"\n",
    "    )\n",
    "    risk_factors: List[str] = Field(\n",
    "        default=[], description=\"Key risk factors mentioned\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Schema for technical specifications\n",
    "class VoltageRange(BaseModel):\n",
    "    min_voltage: Optional[float] = Field(description=\"Minimum voltage\")\n",
    "    max_voltage: Optional[float] = Field(description=\"Maximum voltage\")\n",
    "    unit: str = Field(default=\"V\", description=\"Voltage unit\")\n",
    "\n",
    "\n",
    "class TechnicalSpec(BaseModel):\n",
    "    component_name: str = Field(description=\"Name of the technical component\")\n",
    "    manufacturer: Optional[str] = Field(description=\"Manufacturer name\")\n",
    "    part_number: Optional[str] = Field(description=\"Part or model number\")\n",
    "    description: str = Field(description=\"Brief description of the component\")\n",
    "    operating_voltage: Optional[VoltageRange] = Field(\n",
    "        description=\"Operating voltage range\"\n",
    "    )\n",
    "    maximum_current: Optional[float] = Field(\n",
    "        description=\"Maximum current rating in amperes\"\n",
    "    )\n",
    "    key_features: List[str] = Field(\n",
    "        default=[], description=\"Key features and capabilities\"\n",
    "    )\n",
    "    applications: List[str] = Field(default=[], description=\"Typical applications\")\n",
    "\n",
    "\n",
    "print(\"üìã Extraction schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow Summary\n",
    "\n",
    "Let's create a function that demonstrates the complete workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Workflow function defined!\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "from llama_cloud import ExtractConfig\n",
    "\n",
    "\n",
    "async def complete_document_workflow(markdown_content: str):\n",
    "    \"\"\"\n",
    "    Complete workflow: Parse ‚Üí Classify ‚Üí Extract\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting complete workflow\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Classify\n",
    "    print(\"üè∑Ô∏è  Step 2: Classifying document...\")\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        mode=\"w\", suffix=\".md\", delete=False, encoding=\"utf-8\"\n",
    "    ) as tmp:\n",
    "        tmp.write(markdown_content)\n",
    "        temp_path = Path(tmp.name)\n",
    "\n",
    "    print(temp_path)\n",
    "\n",
    "    classification = await classify_client.aclassify_file_path(\n",
    "        rules=classification_rules, file_input_path=str(temp_path)\n",
    "    )\n",
    "    doc_type = classification.items[0].result.type\n",
    "    confidence = classification.items[0].result.confidence\n",
    "    print(f\"   ‚úÖ Classified as: {doc_type} (confidence: {confidence:.2f})\")\n",
    "\n",
    "    # Step 2: Extract based on classification\n",
    "    print(\"üîç Step 3: Extracting structured data using SourceText...\")\n",
    "    source_text = SourceText(\n",
    "        text_content=markdown_content,\n",
    "        filename=f\"{os.path.basename(temp_path)}_markdown.md\",\n",
    "    )\n",
    "\n",
    "    # Choose schema based on classification\n",
    "    if \"financial\" in doc_type.lower():\n",
    "        schema = FinancialMetrics\n",
    "        print(\"   üìä Using FinancialMetrics schema\")\n",
    "    elif \"technical\" in doc_type.lower():\n",
    "        schema = TechnicalSpec\n",
    "        print(\"   üîß Using TechnicalSpec schema\")\n",
    "    else:\n",
    "        schema = FinancialMetrics  # Default fallback\n",
    "        print(\"   üìä Using default FinancialMetrics schema\")\n",
    "\n",
    "    extract_config = ExtractConfig(\n",
    "        extraction_mode=\"BALANCED\",\n",
    "    )\n",
    "\n",
    "    extraction_result = llama_extract.extract(\n",
    "        data_schema=schema, config=extract_config, files=source_text\n",
    "    )\n",
    "\n",
    "    print(\"   ‚úÖ Extraction complete!\")\n",
    "\n",
    "    return {\n",
    "        \"file_path\": temp_path,\n",
    "        \"markdown_length\": len(markdown_content),\n",
    "        \"classification\": doc_type,\n",
    "        \"confidence\": confidence,\n",
    "        \"extracted_data\": extraction_result.data,\n",
    "        \"markdown_sample\": markdown_content[:200] + \"...\"\n",
    "        if len(markdown_content) > 200\n",
    "        else markdown_content,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"üîß Workflow function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Complete Workflow on Both Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting complete workflow\n",
      "============================================================\n",
      "üè∑Ô∏è  Step 2: Classifying document...\n",
      "/var/folders/g6/4b5lpp5974gcpr890ybhbw4r0000gn/T/tmpos3b62tm.md\n",
      "   ‚úÖ Classified as: financial_document (confidence: 1.00)\n",
      "üîç Step 3: Extracting structured data using SourceText...\n",
      "   üìä Using FinancialMetrics schema\n",
      "..   ‚úÖ Extraction complete!\n",
      "\n",
      "============================================================\n",
      "\n",
      "üöÄ Starting complete workflow\n",
      "============================================================\n",
      "üè∑Ô∏è  Step 2: Classifying document...\n",
      "/var/folders/g6/4b5lpp5974gcpr890ybhbw4r0000gn/T/tmpppz9ub_m.md\n",
      "   ‚úÖ Classified as: technical_specification (confidence: 1.00)\n",
      "üîç Step 3: Extracting structured data using SourceText...\n",
      "   üîß Using TechnicalSpec schema\n",
      "   ‚úÖ Extraction complete!\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìã Processed 2 documents successfully!\n"
     ]
    }
   ],
   "source": [
    "# Process both documents through the complete workflow\n",
    "results = []\n",
    "\n",
    "for doc_text in document_texts:\n",
    "    try:\n",
    "        result = await complete_document_workflow(doc_text)\n",
    "        results.append(result)\n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {doc_path}: {str(e)}\")\n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"üìã Processed {len(results)} documents successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà COMPLETE WORKFLOW RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìÑ Document 1: tmpos3b62tm.md\n",
      "   üìä Classification: financial_document (confidence: 1.00)\n",
      "   üìù Markdown length: 1,348,671 characters\n",
      "   üìã Markdown sample: \n",
      "\n",
      "# UNITED STATES\n",
      "# SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "\n",
      "## FORM 10-K\n",
      "\n",
      "(Mark O...\n",
      "   üéØ Extracted fields: 7 fields\n",
      "   ‚Ä¢ company_name: Uber Technologies, Inc.\n",
      "   ‚Ä¢ document_type: Annual Report on Form 10-K\n",
      "   ‚Ä¢ fiscal_year: 2021\n",
      "   ‚Ä¢ revenue_2021: $21,764\n",
      "   ‚Ä¢ net_income_2021: $(496)\n",
      "   ‚Ä¢ key_business_segments: ['Mobility', 'Delivery', 'Freight', 'All Other (including former New Mobility, e-bikes, e-scooters, Advanced Technologies Group and other technology programs)']\n",
      "   ‚Ä¢ risk_factors: [\"The company faces numerous risk factors across its business operations and environment. The COVID-19 pandemic and related mitigation measures have adversely affected parts of the business, including reduced demand for Mobility offerings and creating ongoing uncertainties. The company's operational and financial performance is influenced by competitive pressure in the mobility, delivery, and logistics industries, characterized by well-established alternatives, low barriers to entry, and low switching costs. Driver classification risks exist if Drivers are deemed employees, workers, or quasi-employees rather than independent contractors, exposing the company to legal actions and financial liabilities globally. Competition challenges require the company to sometimes lower fares, offer incentives, and promotions, which impacts profitability. There are significant operating losses historically with substantial future operating expense increases anticipated, and the ability to achieve or maintain profitability is uncertain. Network value depends on maintaining critical mass among Drivers, consumers, merchants, shippers, and carriers, and failures to do so diminish platform attractiveness. Brand and reputation maintenance is critical, with exposure to negative publicity, media coverage, and risks from associated companies' brands or licensed brands in joint ventures.\\n\\nOperational risks include historical workplace culture and compliance challenges, management complexity due to rapid growth, technological infrastructure issues potentially causing disruptions or poor user experience, and security or data privacy breaches that could impact revenue and reputation. Platform users may engage in or be subjected to criminal, violent, or dangerous activity leading to safety incidents and legal actions. New offerings and technologies investments are inherently risky without guaranteed benefits. Economic conditions, inflation, and increased costs (fuel, food, labor, energy) may negatively impact results. Regulatory risks are extensive and global, involving payment and financial services compliance, licensing, anti-money laundering laws, data privacy (GDPR, CCPA, LGPD), and labor laws. Legal and regulatory investigations and inquiries, including antitrust, FCPA, labor classification, data protection, and intellectual property matters, pose risks of fines, penalties, operational changes, and increased costs.\\n\\nGeopolitical and jurisdictional risks include operating limitations or bans in some locations, currency exchange risk, and complex evolving regulations with the potential for fines and loss of licenses or permits. Insurance risks include potential inadequacy of reserves, liability exposure from accidents or impersonation, and insurer insolvency. Driver qualification requirements and background checks may increase costs or fail to expose all relevant information, with associated insurance cost risks and potential for courtroom or regulatory challenges to pricing models.\\n\\nFinancial risks comprise significant accumulated deficits, requirement for additional capital with uncertain availability, debt obligations, tax exposure including uncertain positions and observed changes in tax laws, and volatility in common stock price with no expected cash dividends. Accounting judgments and estimates involve critical assumptions affecting reported financial metrics related to goodwill, revenue recognition, incentive accruals, and stock-based compensation. Cybersecurity risks include exposures to malware, ransomware, phishing, and other cyberattacks. Climate change presents physical and transitional risks that may impact operations and costs, and failure to meet climate commitments may have operational and reputational consequences.\\n\\nOther risks include potential liability under anti-corruption and anti-terrorism laws, adverse effects from defaults under debt agreements, limitations in takeover actions due to corporate governance provisions, and the impact of non-GAAP financial measure limitations. Overall, these diverse and interconnected risk factors contribute to significant uncertainty regarding the company's future business prospects, operating results, and financial condition.\"]\n",
      "\n",
      "üìÑ Document 2: tmpppz9ub_m.md\n",
      "   üìä Classification: technical_specification (confidence: 1.00)\n",
      "   üìù Markdown length: 90,971 characters\n",
      "   üìã Markdown sample: \n",
      "\n",
      "LM317\n",
      "SLVS044Z ‚Äì SEPTEMBER 1997 ‚Äì REVISED APRIL 2025\n",
      "\n",
      "# LM317 3-Pin Adjustable Regulator\n",
      "\n",
      "## 1 Fea...\n",
      "   üéØ Extracted fields: 8 fields\n",
      "   ‚Ä¢ component_name: LM317\n",
      "   ‚Ä¢ manufacturer: Texas Instruments\n",
      "   ‚Ä¢ part_number: LM317\n",
      "   ‚Ä¢ description: The LM317 is an adjustable three-pin, positive-voltage regulator capable of supplying up to 1.5A over an output voltage range of 1.25V to 37V. It features line and load regulation, internal current limiting, thermal overload protection, and safe operating area compensation.\n",
      "   ‚Ä¢ operating_voltage: {'min_voltage': 1.25, 'max_voltage': 37.0, 'unit': 'V'}\n",
      "   ‚Ä¢ maximum_current: 1.5\n",
      "   ‚Ä¢ key_features: ['Adjustable output voltage: 1.25V to 37V', 'Output current up to 1.5A', 'Line regulation: 0.01%/V (typical)', 'Load regulation: 0.1% (typical)', 'Internal short-circuit current limiting', 'Thermal overload protection', 'Output safe-area compensation', 'High power supply rejection ratio (PSRR): 80dB at 120Hz (new chip)', 'Available in SOT-223, TO-263, and TO-220 packages']\n",
      "   ‚Ä¢ applications: ['Multifunction printers', 'AC drive power stage modules', 'Electricity meters', 'Servo drive control modules', 'Merchant network and server power supply units']\n",
      "\n",
      "‚ú® Workflow completed successfully!\n",
      "\n",
      "üìö Key Learnings:\n",
      "   ‚Ä¢ Parse: Converted documents to clean markdown format\n",
      "   ‚Ä¢ Classify: Automatically categorized document types\n",
      "   ‚Ä¢ Extract: Used SourceText with markdown for structured data extraction\n",
      "   ‚Ä¢ The markdown content provides much better context for extraction than raw PDFs\n"
     ]
    }
   ],
   "source": [
    "print(\"üìà COMPLETE WORKFLOW RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nüìÑ Document {i}: {os.path.basename(result['file_path'])}\")\n",
    "    print(\n",
    "        f\"   üìä Classification: {result['classification']} (confidence: {result['confidence']:.2f})\"\n",
    "    )\n",
    "    print(f\"   üìù Markdown length: {result['markdown_length']:,} characters\")\n",
    "    print(f\"   üìã Markdown sample: {result['markdown_sample'][:100]}...\")\n",
    "    print(f\"   üéØ Extracted fields: {len(result['extracted_data'])} fields\")\n",
    "\n",
    "    # Print all key‚Äìvalue pairs\n",
    "    extracted = result[\"extracted_data\"]\n",
    "    for key, value in extracted.items():\n",
    "        print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚ú® Workflow completed successfully!\")\n",
    "print(\"\\nüìö Key Learnings:\")\n",
    "print(\"   ‚Ä¢ Parse: Converted documents to clean markdown format\")\n",
    "print(\"   ‚Ä¢ Classify: Automatically categorized document types\")\n",
    "print(\"   ‚Ä¢ Extract: Used SourceText with markdown for structured data extraction\")\n",
    "print(\n",
    "    \"   ‚Ä¢ The markdown content provides much better context for extraction than raw PDFs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete **Parse ‚Üí Classify ‚Üí Extract** workflow using LlamaCloud services:\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **LlamaParse** (`llama_cloud_services.parse.base.LlamaParse`):\n",
    "   - Converts documents to clean, structured markdown\n",
    "   - Preserves document structure and formatting\n",
    "   - Handles various file types (PDF, DOCX, etc.)\n",
    "\n",
    "2. **ClassifyClient** (`llama_cloud_services.beta.classifier.client.ClassifyClient`):\n",
    "   - Automatically categorizes documents based on content\n",
    "   - Uses customizable rules for classification\n",
    "   - Provides confidence scores for classifications\n",
    "\n",
    "3. **LlamaExtract with SourceText** (`llama_cloud_services.extract.extract.LlamaExtract`, `SourceText`):\n",
    "   - Extracts structured data using custom Pydantic schemas\n",
    "   - **SourceText** allows using markdown content as input instead of raw files\n",
    "   - Provides much better extraction accuracy when using processed markdown\n",
    "\n",
    "### Workflow Benefits:\n",
    "\n",
    "- **Better Accuracy**: Using markdown from parsing provides cleaner, more structured input for extraction\n",
    "- **Automatic Routing**: Classification allows different processing logic for different document types\n",
    "- **Structured Output**: Custom schemas ensure consistent, structured data extraction\n",
    "- **Flexible Input**: SourceText supports text content, file paths, and bytes\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **SourceText is the bridge**: It allows you to pass the clean markdown content from parsing directly to extraction\n",
    "2. **Markdown improves extraction**: Pre-processed markdown provides much better context than raw PDFs\n",
    "3. **Classification enables smart routing**: Different document types can use different extraction schemas\n",
    "4. **End-to-end automation**: The entire workflow can be automated for production use\n",
    "\n",
    "This approach is ideal for production document processing pipelines where you need to:\n",
    "- Process various document types automatically\n",
    "- Extract structured data consistently\n",
    "- Maintain high accuracy and reliability\n",
    "- Handle documents at scale\n",
    "\n",
    "The combination of these three services provides a powerful, flexible document processing pipeline that can handle complex, real-world document processing requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
